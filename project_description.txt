Project Title: Real-Time Wireframe-Based Pothole Detection and Avoidance System
Developer: Mohamed B Sirajuddeen
Domain: Artificial Intelligence, Autonomous Vehicles, 3D Reconstruction, Computer Vision
1. INTRODUCTION (Extracted & Consolidated)

Autonomous vehicles rely heavily on visual perception systems for safe navigation. In developing countries, inconsistent road conditions—frequent potholes, cracks, and surface deformities—create major challenges. Existing solutions mostly depend on LiDAR and high-end sensors, which increase cost and limit real-world deployment.

To address this gap, the system aims to build a low-cost, camera-only real-time pothole detection setup capable of generating 3D wireframe meshes from RGB input, making it suitable for AVs and road maintenance.

2. OBJECTIVE

Develop a vision-only real-time pothole detection and measurement system.

Replace costly LiDAR-based depth systems with monocular depth estimation.

Create a 3D mesh/wireframe of the road surface for accurate geometry analysis.

Enable real-time autonomous decisions such as slow-down or avoidance.

3. PROBLEM STATEMENT

Existing pothole detection systems achieve high accuracy through Camera + LiDAR fusion, but:

LiDAR is expensive and unsuitable for mass adoption.

Performance drops in wet or reflective conditions.

High calibration & computational overhead.

Not feasible for embedded or low-cost platforms.

There is a need for a cost-effective, lightweight, high-performance, camera-only system capable of real-time pothole detection and analysis.

4. EXISTING SYSTEM – SUMMARY

Current methods use:

LiDAR + RGB Camera fusion

YOLO object detection

GNSS for geo-tagging

3D point cloud reconstruction

Convex Hull for geometry extraction

Advantages: High detection accuracy, automation, robustness.
Drawbacks: High cost, heavy computation, sensor dependency.

5. LITERATURE SURVEY (Condensed for Environmental Setup)

The following technologies influence the system’s environmental requirements:

Detection Models

YOLOv8 variants (YOLOv8n, YOLOv8-Seg)

Depth Estimation

MiDaS

DepthAnything V2

Mobile3DRecon (inspired for incremental mesh recon)

Mesh & Geometry

Open3D

MLS surface smoothing

GPU-optimized triangulation approaches

Key Takeaways for Setup

High GPU utilization for detection + depth estimation

CPU fallback possible using optimized mesh generation

Need strong environment for Python + PyTorch + CUDA

Robust real-time video processing pipeline

6. PROPOSED SOLUTION (Rewritten for Project File)

A vision-based real-time perception pipeline that:

Uses a monocular RGB camera for live road capture

Detects anomalies with YOLOv8-Seg

Predicts depth with MiDaS / DepthAnything V2

Generates 3D point clouds + meshes using Open3D

Computes depth, width, area, perimeter, volume

Triggers decisions (slow-down, micro-avoidance, logging)

This forms the baseline for designing the environment setup and required software-hardware stack.

7. PROPOSED SYSTEM WORKFLOW (Text Version)

Step-by-step pipeline:

Input Capture – Continuous RGB frames from monocular camera

Detection & Segmentation – YOLOv8 for potholes/cracks/bump masks

Depth Estimation – MiDaS generating dense depth maps

Point Cloud Construction – Mask + Depth → local 3D structure

Wireframe/Mesh Generation – Open3D triangulated mesh

Geometry Analysis – Depth, area, perimeter, volume

Decision Engine – Speed change / steering / logging

8. PROPOSED SYSTEM ARCHITECTURE (Textual Architecture View)

Camera Module
→ RGB Video Stream

Preprocessing Module
→ Frame Normalization
→ Resolution Adjustment

Anomaly Detection Module
→ YOLOv8-Seg
→ Pixel-Level Mask Generation

Depth Estimation Module
→ MiDaS / DepthAnything
→ Depth Map Generation

3D Reconstruction Module
→ Open3D Mesh Generation
→ Point Cloud Integration

Geometry Analyzer
→ Depth, Width, Perimeter, Volume Calculation

Decision Engine
→ Alerts, Slowdown, Avoidance

Output Interface
→ Visualization
→ Log Export

9. ENVIRONMENTAL SETUP REQUIREMENTS
9.1 Hardware Requirements

Minimum Specs:

CPU: Intel i5 / Ryzen 5

RAM: 8 GB

Storage: 256 GB SSD

GPU: Optional (CPU mode possible but slower)

Camera: HD USB Camera (720p or 1080p)

Recommended Specs (Real-Time Pipeline):

CPU: Intel i7 / Ryzen 7 or higher

RAM: 16–32 GB

GPU: NVIDIA RTX 2060 / 3060 / 4060

CUDA Capability: 11.7+

Camera: Full HD 1080p 60fps

9.2 Software Requirements

Operating System

Ubuntu 20.04 / 22.04 (recommended)

Windows 10 / 11 (optional)

Programming

Python 3.10+

PyTorch

CUDA Toolkit (if using GPU)

Anaconda (for environment isolation)

Jupyter Notebook or VS Code

Python Libraries Required

ultralytics (YOLOv8)

torch

torchvision

opencv-python

open3d

numpy

matplotlib

scipy

Environment Setup Template (For Project File)
conda create -n pothole_env python=3.10  
conda activate pothole_env

pip install ultralytics  
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  
pip install opencv-python  
pip install open3d  
pip install matplotlib numpy scipy  

10. DATASET REQUIREMENTS (Extracted Context)

RGB road images (normal + pothole)

Segmentation masks (optional for training)

Depth data (auto-generated via MiDaS)

Annotation format compatible with YOLOv8

11. CURRENT STATUS (Text Version)

Base paper selected

Existing architectures analyzed

Problem statement finalized

Literature survey completed

Technology stack identified (YOLOv8 + MiDaS + Open3D)

Integration research in progress

Dataset collection plan prepared

Proposed system architecture design refined

12. CONCLUSION (Shortened)

This environmental setup outlines the complete foundation for developing the real-time camera-only pothole detection system. With a structured pipeline, selected AI models, and a clear hardware–software stack, the project is ready for implementation and scaling.

✅ Delivered: Clean, Organized, Report-ready Text Based on Your PPT

Boss, if you want, I can also generate:

✅ Environmental Setup chapter for your final report
✅ System Architecture diagrams (ASCII or editable formats)
✅ Flowcharts, block diagrams, or pseudo-code
✅ Deployment instructions for GPU/CPU systems
✅ Future enhancement section